{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c85e107-e5b4-40fa-ba55-7448e36626fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf, col\n",
    "# from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import pyspark\n",
    "from IPython.display import clear_output, display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "from pyspark.sql.types import DateType, IntegerType, StringType, StructType\n",
    "from pyspark.sql.functions import min, max, avg\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01eefe43-4698-412c-b19f-08027aff8954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 10:01:58 WARN Utils: Your hostname, all-MS-7D35 resolves to a loopback address: 127.0.1.1; using 192.168.1.108 instead (on interface enp2s0)\n",
      "23/05/18 10:01:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/18 10:01:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/18 10:02:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(\"Assignment_5.3\").getOrCreate())\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Assignment_5.3\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501b280e-58a9-4ea3-8f17-10d254a532d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data from source to DataFrame\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"True\").option(\"inferschema\", \"true\").load(\"Documents/Assignment_5.3/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0913a3-da0a-4afc-8149-44efa250438c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 10: integer (nullable = true)\n",
      " |-- 01: integer (nullable = true)\n",
      " |-- 3: integer (nullable = true)\n",
      " |-- Braund, Mr. Owen Harris: string (nullable = true)\n",
      " |-- male: string (nullable = true)\n",
      " |-- 22: integer (nullable = true)\n",
      " |-- 16: integer (nullable = true)\n",
      " |-- 07: integer (nullable = true)\n",
      " |-- A/5 21171: string (nullable = true)\n",
      " |-- 7.25: double (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- S: string (nullable = true)\n",
      " |-- 2020-01-01 13:45:25: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc721b0-77a7-4583-a364-fea08606aca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------------------+------+----+---+---+----------------+-------+----+---+-------------------+\n",
      "| 10| 01|  3|Braund, Mr. Owen Harris|  male|  22| 16| 07|       A/5 21171|   7.25|_c10|  S|2020-01-01 13:45:25|\n",
      "+---+---+---+-----------------------+------+----+---+---+----------------+-------+----+---+-------------------+\n",
      "|  2|  1|  1|   Cumings, Mrs. Joh...|female|  38|  1|  0|        PC 17599|71.2833| C85|  C|2020-01-01 13:44:48|\n",
      "|  3|  1|  3|   Heikkinen, Miss. ...|female|  26|  0|  0|STON/O2. 3101282|  7.925|null|  S|2020-01-01 13:38:11|\n",
      "|  4|  1|  1|   Futrelle, Mrs. Ja...|female|  35|  1|  0|          113803|   53.1|C123|  S|2020-01-01 13:32:00|\n",
      "|  5|  0|  3|   Allen, Mr. Willia...|  male|  35|  0|  0|          373450|   8.05|null|  S|2020-01-01 13:36:30|\n",
      "|  6|  0|  3|       Moran, Mr. James|  male|null|  0|  0|          330877| 8.4583|null|  Q|2020-01-01 13:31:39|\n",
      "|  7|  0|  1|   McCarthy, Mr. Tim...|  male|  54|  0|  0|           17463|51.8625| E46|  S|2020-01-01 13:37:31|\n",
      "|  8|  0|  3|   Palsson, Master. ...|  male|   2|  3|  1|          349909| 21.075|null|  S|2020-01-01 13:49:08|\n",
      "|  9|  1|  3|   Johnson, Mrs. Osc...|female|  27|  0|  2|          347742|11.1333|null|  S|2020-01-01 13:33:42|\n",
      "| 10|  1|  2|   Nasser, Mrs. Nich...|female|  14|  1|  0|          237736|30.0708|null|  C|2020-01-01 13:32:53|\n",
      "| 11|  1|  3|   Sandstrom, Miss. ...|female|   4|  1|  1|         PP 9549|   16.7|  G6|  S|2020-01-01 13:32:23|\n",
      "| 12|  1|  1|   Bonnell, Miss. El...|female|  58|  0|  0|          113783|  26.55|C103|  S|2020-01-01 13:30:12|\n",
      "| 13|  0|  3|   Saundercock, Mr. ...|  male|  20|  0|  0|       A/5. 2151|   8.05|null|  S|2020-01-01 13:33:34|\n",
      "| 14|  0|  3|   Andersson, Mr. An...|  male|  39|  1|  5|          347082| 31.275|null|  S|2020-01-01 13:30:20|\n",
      "| 15|  0|  3|   Vestrom, Miss. Hu...|female|  14|  0|  0|          350406| 7.8542|null|  S|2020-01-01 13:41:17|\n",
      "| 16|  1|  2|   Hewlett, Mrs. (Ma...|female|  55|  0|  0|          248706|   16.0|null|  S|2020-01-01 13:34:22|\n",
      "| 17|  0|  3|   Rice, Master. Eugene|  male|   2|  4|  1|          382652| 29.125|null|  Q|2020-01-01 13:41:55|\n",
      "| 18|  1|  2|   Williams, Mr. Cha...|  male|null|  0|  0|          244373|   13.0|null|  S|2020-01-01 13:39:35|\n",
      "| 19|  0|  3|   Vander Planke, Mr...|female|  31|  1|  0|          345763|   18.0|null|  S|2020-01-01 13:39:38|\n",
      "| 20|  1|  3|   Masselmani, Mrs. ...|female|null|  0|  0|            2649|  7.225|null|  C|2020-01-01 13:36:56|\n",
      "| 21|  0|  2|   Fynney, Mr. Joseph J|  male|  35|  0|  0|          239865|   26.0|null|  S|2020-01-01 13:48:28|\n",
      "+---+---+---+-----------------------+------+----+---+---+----------------+-------+----+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 10:02:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 0, 3, Braund, Mr. Owen Harris, male, 22, 1, 0, A/5 21171, 7.25, , S, 2020-01-01 13:45:25\n",
      " Schema: 10, 01, 3, Braund, Mr. Owen Harris, male, 22, 16, 07, A/5 21171, 7.25, _c10, S, 2020-01-01 13:45:25\n",
      "Expected: 10 but found: 1\n",
      "CSV file: file:///home/syedmuhammadraqimali/Documents/Assignment_5.3/titanic.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09fd83be-98fd-442a-9cea-b721d11a61fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_column_names = df.columns\n",
    "\n",
    "col_names=[\"PassengerId\",\"Survived\",\"Pclass\",\"Name\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Ticket\",\"Fare\",\"Cabin\",\"Embarked\",\"Date\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a4cdb6-e894-427a-a86b-e65d75304551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(old_column_names)):\n",
    "    df = df.withColumnRenamed(old_column_names[i], col_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d14536-d390-4626-919f-24f898413227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "789a78bf-25c0-4625-9a65-ed31b4f855ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate statistics for numerical columns\n",
    "\n",
    "\n",
    "numerical_cols=[]\n",
    "\n",
    "for col_name, data_type in df.dtypes:\n",
    "    if data_type in [\"int\", \"bigint\", \"double\", \"float\"]:\n",
    "        numerical_cols.append(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db79b959-f2a4-44bc-9e66-5dc7e7a706d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbc3d9ab-28c0-417b-92b0-4e67a714b2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 10:02:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/05/18 10:02:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 0, 3, 22, 1, 0, 7.25\n",
      " Schema: 10, 01, 3, 22, 16, 07, 7.25\n",
      "Expected: 10 but found: 1\n",
      "CSV file: file:///home/syedmuhammadraqimali/Documents/Assignment_5.3/titanic.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numerical_stats = df.select(*numerical_cols).describe().toPandas()\n",
    "numerical_stats = numerical_stats.set_index(\"summary\")\n",
    "numerical_stats = numerical_stats.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f00b7e70-3a81-401a-aa1d-0dbbe6f3073f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>713.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.500000</td>\n",
       "      <td>0.384270</td>\n",
       "      <td>2.307865</td>\n",
       "      <td>29.690042</td>\n",
       "      <td>0.522472</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>32.232246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stddev</th>\n",
       "      <td>257.065167</td>\n",
       "      <td>0.486696</td>\n",
       "      <td>0.836220</td>\n",
       "      <td>14.543836</td>\n",
       "      <td>1.103247</td>\n",
       "      <td>0.806409</td>\n",
       "      <td>49.714317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "summary                                                                \n",
       "count     890.000000  890.000000  890.000000  713.000000  890.000000   \n",
       "mean      446.500000    0.384270    2.307865   29.690042    0.522472   \n",
       "stddev    257.065167    0.486696    0.836220   14.543836    1.103247   \n",
       "min         2.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "max       891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "              Parch        Fare  \n",
       "summary                          \n",
       "count    890.000000  890.000000  \n",
       "mean       0.382022   32.232246  \n",
       "stddev     0.806409   49.714317  \n",
       "min        0.000000    0.000000  \n",
       "max        6.000000  512.329200  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac4caa-bfa3-4789-9915-2f952b710c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a1b8060-54db-4117-838c-4fe5d34627f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 10:02:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 0, 3, 22, 1, 0, 7.25\n",
      " Schema: 10, 01, 3, 22, 16, 07, 7.25\n",
      "Expected: 10 but found: 1\n",
      "CSV file: file:///home/syedmuhammadraqimali/Documents/Assignment_5.3/titanic.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------+-------+---------+---------+--------+---------------+------------+----------+-------+---------+---------+--------+---------------+------------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|min_PassengerId|min_Survived|min_Pclass|min_Age|min_SibSp|min_Parch|min_Fare|max_PassengerId|max_Survived|max_Pclass|max_Age|max_SibSp|max_Parch|max_Fare|avg_PassengerId|      avg_Survived|        avg_Pclass|           avg_Age|         avg_SibSp|          avg_Parch|         avg_Fare|\n",
      "+---------------+------------+----------+-------+---------+---------+--------+---------------+------------+----------+-------+---------+---------+--------+---------------+------------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|              2|           0|         1|      0|        0|        0|     0.0|            891|           1|         3|     80|        8|        6|512.3292|          446.5|0.3842696629213483|2.3078651685393257|29.690042075736326|0.5224719101123596|0.38202247191011235|32.23224640449435|\n",
      "+---------------+------------+----------+-------+---------+---------+--------+---------------+------------+----------+-------+---------+---------+--------+---------------+------------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate min, max, and avg for numerical columns using Spark functions\n",
    "df.select([min(col).alias(\"min_\" + col) for col in numerical_cols] + \\\n",
    "          [max(col).alias(\"max_\" + col) for col in numerical_cols] + \\\n",
    "          [avg(col).alias(\"avg_\" + col) for col in numerical_cols]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c40e29-bc26-4439-ac3a-c81d57639ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 10:02:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1, 0, 3, Braund, Mr. Owen Harris, male, 22, 1, 0, A/5 21171, 7.25, , S, 2020-01-01 13:45:25\n",
      " Schema: 10, 01, 3, Braund, Mr. Owen Harris, male, 22, 16, 07, A/5 21171, 7.25, _c10, S, 2020-01-01 13:45:25\n",
      "Expected: 10 but found: 1\n",
      "CSV file: file:///home/syedmuhammadraqimali/Documents/Assignment_5.3/titanic.csv\n",
      "23/05/18 10:02:08 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 7)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_121997/1649325756.py\", line 4, in change_last_letter_after_space\n",
      "AttributeError: 'int' object has no attribute 'split'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/05/18 10:02:08 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 7) (192.168.1.108 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_121997/1649325756.py\", line 4, in change_last_letter_after_space\n",
      "AttributeError: 'int' object has no attribute 'split'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/05/18 10:02:08 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_121997/1649325756.py\", line 4, in change_last_letter_after_space\nAttributeError: 'int' object has no attribute 'split'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(column, change_last_letter_udf(df[column]))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Show the modified DataFrame\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_121997/1649325756.py\", line 4, in change_last_letter_after_space\nAttributeError: 'int' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "str_columns = [\"PassengerId\",\"Sex\",\"Cabin\",\"Embarked\"]\n",
    "def change_last_letter_after_space(word):\n",
    "    if word is not None:\n",
    "        words = word.split()\n",
    "        for i in range(len(words)):\n",
    "            words[i] = words[i][:-1] + \"1\"\n",
    "        return \" \".join(words)\n",
    "    return word\n",
    "change_last_letter_udf = udf(change_last_letter_after_space, StringType())\n",
    "# Apply the UDF to each categorical column\n",
    "for column in str_columns:\n",
    "    df = df.withColumn(column, change_last_letter_udf(df[column]))\n",
    "# Show the modified DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315ebb7-3d91-4a0a-b850-c63e93de1f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
